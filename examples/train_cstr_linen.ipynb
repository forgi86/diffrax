{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import diffrax\n",
    "from flax import linen as nn\n",
    "from typing import Sequence, Dict, Any\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "from interpolation import ZOHInterpolation\n",
    "import nonlinear_benchmarks\n",
    "import nonlinear_benchmarks.error_metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = nonlinear_benchmarks.Cascaded_Tanks(atleast_2d=True)\n",
    "sampling_time = train_val.sampling_time\n",
    "u_train, y_train = train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale data\n",
    "scaler_u = StandardScaler()\n",
    "u = scaler_u.fit_transform(u_train).astype(jnp.float32)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y_train).astype(jnp.float32)\n",
    "\n",
    "ts = jnp.arange(0.0, u.shape[0]) * sampling_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 2\n",
    "nu = u.shape[-1]\n",
    "ny = y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    layer_kwargs: Dict[str, Any] = None\n",
    "    last_layer_kwargs: Dict[str, Any] = None\n",
    "\n",
    "    def setup(self):\n",
    "        layer_kwargs = self.layer_kwargs if self.layer_kwargs is not None else {}\n",
    "        last_layer_kwargs = self.last_layer_kwargs if self.last_layer_kwargs is not None else {}\n",
    "        self.layers = [nn.Dense(feat, **layer_kwargs) for feat in self.features[:-1]] + [nn.Dense(self.features[-1], **last_layer_kwargs)]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.tanh(x)\n",
    "        return x\n",
    "    \n",
    "class StateUpdateMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    scale: float = 1e-3\n",
    "\n",
    "    def setup(self):\n",
    "        # Set custom initializers\n",
    "        #kernel_init = jax.nn.initializers.normal(stddev=1e-4)  # Standard deviation for the normal distribution\n",
    "        #bias_init = jax.nn.initializers.constant(0)  # Constant value for all biases\n",
    "\n",
    "        # Create layers with custom initializers\n",
    "        self.net = MLP(self.features)#, last_layer_kwargs={\"kernel_init\": kernel_init, \"bias_init\": bias_init})\n",
    "\n",
    "    def __call__(self, x, u):\n",
    "        dx = self.scale * self.net(jnp.r_[x, u])\n",
    "        return dx  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xu = StateUpdateMLP(features=[32, 16, nx])\n",
    "g_x = MLP(features=[16, ny])\n",
    "x0 = jnp.zeros(nx)\n",
    "_, params_f = f_xu.init_with_output(jax.random.key(0), jnp.ones(nx), jnp.ones(nu))\n",
    "_, params_g = g_x.init_with_output(jax.random.key(0), jnp.ones(nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(params_f, params_g, x0, u):\n",
    "    #u_fun = diffrax.LinearInterpolation(ts=ts, ys=u.ravel())\n",
    "    u_fun = ZOHInterpolation(ts=ts, ys=u.ravel())\n",
    "    def vector_field(t, x, args):\n",
    "        ut = u_fun.evaluate(t)[..., None]\n",
    "        dx = f_xu.apply(args, x, ut)\n",
    "        return dx\n",
    "\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(vector_field),\n",
    "        #diffrax.Euler(),\n",
    "        #diffrax.Tsit5(),\n",
    "        diffrax.Dopri5(),\n",
    "        ts[0],\n",
    "        ts[-1],\n",
    "        dt0=sampling_time,\n",
    "        y0=x0,\n",
    "        #stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6, jump_ts=ts),\n",
    "        stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "        saveat=diffrax.SaveAt(ts=ts),\n",
    "        args=params_f,\n",
    "        max_steps=int(1e6),\n",
    "    )\n",
    "    x = sol.ys\n",
    "    y = g_x.apply(params_g, x)\n",
    "    #y = x[:, 1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_variables = (params_f, params_g, x0)\n",
    "def loss_fn(opt_variables, u, y):\n",
    "    params_f, params_g, x0 = opt_variables\n",
    "    y_pred = simulate(params_f, params_g, x0, u)\n",
    "    return jnp.mean((y - y_pred) ** 2)\n",
    "\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "opt_state = optimizer.init(opt_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "time_start = time.time()\n",
    "LOSS = []\n",
    "epochs = 10_000\n",
    "for epoch in (pbar := tqdm(range(epochs))):\n",
    "    loss_val, grads = loss_grad_fn(opt_variables, u, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    opt_variables = optax.apply_updates(opt_variables, updates)\n",
    "    LOSS.append(loss_val)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix_str(f\"Loss step {epoch}: {loss_val}\")\n",
    "    #print()\n",
    "\n",
    "train_time = time.time() - time_start\n",
    "print(f\"Training time: {train_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_f, params_g, x0 = opt_variables\n",
    "y_sim = simulate(params_f, params_g, x0, u)\n",
    "y_sim.shape\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "plt.plot(y_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_test, y_test = test\n",
    "\n",
    "u_test = scaler_u.transform(u_test)\n",
    "y_test_hat = simulate(params_f, params_g, x0, u_test)\n",
    "y_test_hat = scaler_y.inverse_transform(y_test_hat)\n",
    "\n",
    "fit = metrics.fit_index(y_test, y_test_hat)[0]\n",
    "rmse = metrics.RMSE(y_test, y_test_hat)[0] \n",
    "nrmse = metrics.NRMSE(y_test, y_test_hat)[0]\n",
    "\n",
    "print(f\"{fit=} \\n{rmse=} \\n{nrmse=}\")\n",
    "plt.figure()\n",
    "plt.plot(y_test)\n",
    "plt.plot(y_test_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
